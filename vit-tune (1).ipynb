{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10893497,"sourceType":"datasetVersion","datasetId":6769814}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm transformers --quiet\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-02T03:29:49.458929Z","iopub.execute_input":"2025-03-02T03:29:49.459186Z","iopub.status.idle":"2025-03-02T03:29:54.182188Z","shell.execute_reply.started":"2025-03-02T03:29:49.459151Z","shell.execute_reply":"2025-03-02T03:29:54.181071Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport timm\nimport os\nfrom PIL import Image\nimport numpy as np\n\n# Config\nBATCH_SIZE = 16\nIMG_SIZE = 224\nNUM_CLASSES = 200\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nNUM_EPOCHS = 20\nLEARNING_RATE = 1e-4\nWARMUP_EPOCHS = 3\nPATIENCE = 4  # Early stopping patience\n\n# Paths (update to your dataset location in Kaggle)\nTRAIN_IMAGES_PATH = \"/kaggle/input/dung166/Train/Train\"\nTRAIN_LABELS_PATH = \"/kaggle/input/dung166/train.txt\"\nTEST_IMAGES_PATH = \"/kaggle/input/dung166/Test/Test\"\nTEST_LABELS_PATH = \"/kaggle/input/dung166/test.txt\"\n\n# Dataset\nclass CUB200Dataset(Dataset):\n    def __init__(self, images_path, labels_path, transform=None):\n        self.images_path = images_path\n        self.transform = transform\n        self.image_labels = []\n        with open(labels_path, 'r') as file:\n            for line in file.readlines():\n                image_name, label = line.strip().split()\n                self.image_labels.append((image_name, int(label)))\n\n    def __len__(self):\n        return len(self.image_labels)\n\n    def __getitem__(self, idx):\n        img_name, label = self.image_labels[idx]\n        img_path = os.path.join(self.images_path, img_name)\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# Transforms\ntransform_train = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\n# Datasets and Loaders\ntrain_dataset = CUB200Dataset(TRAIN_IMAGES_PATH, TRAIN_LABELS_PATH, transform=transform_train)\ntest_dataset = CUB200Dataset(TEST_IMAGES_PATH, TEST_LABELS_PATH, transform=transform_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\n# Model\nmodel = timm.create_model('vit_base_patch8_224', pretrained=True, num_classes=NUM_CLASSES)\nmodel = model.to(DEVICE)\n\n# Loss, Optimizer, Scheduler\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n\n# Early Stopping Variables\nbest_top1 = 0.0\nepochs_without_improvement = 0\n\n# Training Function\ndef train(model, loader, epoch):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    for images, labels in loader:\n        images, labels = images.to(DEVICE), labels.to(DEVICE)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(labels).sum().item()\n        total += labels.size(0)\n\n    return running_loss / len(loader), correct / total\n\n# Evaluation Function\ndef evaluate(model, loader):\n    model.eval()\n    correct, total = 0, 0\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(DEVICE), labels.to(DEVICE)\n            outputs = model(images)\n            _, predicted = outputs.max(1)\n\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n            correct += predicted.eq(labels).sum().item()\n            total += labels.size(0)\n\n    top1_acc = correct / total\n\n    # Average Class Accuracy\n    all_preds, all_labels = np.array(all_preds), np.array(all_labels)\n    class_accuracies = []\n    for cls in range(NUM_CLASSES):\n        cls_indices = np.where(all_labels == cls)[0]\n        if len(cls_indices) > 0:\n            cls_correct = (all_preds[cls_indices] == all_labels[cls_indices]).sum()\n            class_accuracies.append(cls_correct / len(cls_indices))\n\n    avg_class_acc = np.mean(class_accuracies)\n\n    return top1_acc, avg_class_acc\n\n# Warmup LR Adjuster\ndef adjust_learning_rate(optimizer, epoch):\n    if epoch < WARMUP_EPOCHS:\n        lr = LEARNING_RATE * (epoch + 1) / WARMUP_EPOCHS\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n# Training Loop with Early Stopping\nfor epoch in range(NUM_EPOCHS):\n    adjust_learning_rate(optimizer, epoch)\n\n    train_loss, train_acc = train(model, train_loader, epoch)\n    top1_acc, avg_class_acc = evaluate(model, test_loader)\n\n    scheduler.step()\n\n    # Save best model & apply early stopping logic\n    if top1_acc > best_top1:\n        best_top1 = top1_acc\n        torch.save(model.state_dict(), 'best_model.pth')\n        epochs_without_improvement = 0\n    else:\n        epochs_without_improvement += 1\n\n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - \"\n          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n          f\"Top-1 Acc: {top1_acc:.4f}, Avg Class Acc: {avg_class_acc:.4f}\")\n\n    if epochs_without_improvement >= PATIENCE:\n        print(f\"Early stopping triggered after {epoch+1} epochs!\")\n        break\n\n# Final Evaluation\nmodel.load_state_dict(torch.load('best_model.pth'))\ntop1_acc, avg_class_acc = evaluate(model, test_loader)\nprint(f\"\\nFinal Top-1 Accuracy: {top1_acc:.4f}\")\nprint(f\"Final Average Per-Class Accuracy: {avg_class_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T06:59:14.313777Z","iopub.execute_input":"2025-03-02T06:59:14.314099Z","iopub.status.idle":"2025-03-02T10:09:55.838963Z","shell.execute_reply.started":"2025-03-02T06:59:14.314075Z","shell.execute_reply":"2025-03-02T10:09:55.837829Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20 - Train Loss: 3.4103, Train Acc: 0.3835, Top-1 Acc: 0.7550, Avg Class Acc: 0.7480\nEpoch 2/20 - Train Loss: 1.5637, Train Acc: 0.8142, Top-1 Acc: 0.7674, Avg Class Acc: 0.7634\nEpoch 3/20 - Train Loss: 1.3308, Train Acc: 0.8969, Top-1 Acc: 0.7774, Avg Class Acc: 0.7735\nEpoch 4/20 - Train Loss: 1.1380, Train Acc: 0.9586, Top-1 Acc: 0.7998, Avg Class Acc: 0.7960\nEpoch 5/20 - Train Loss: 1.0213, Train Acc: 0.9872, Top-1 Acc: 0.8339, Avg Class Acc: 0.8322\nEpoch 6/20 - Train Loss: 0.9604, Train Acc: 0.9957, Top-1 Acc: 0.8713, Avg Class Acc: 0.8691\nEpoch 7/20 - Train Loss: 0.9288, Train Acc: 0.9977, Top-1 Acc: 0.8729, Avg Class Acc: 0.8726\nEpoch 8/20 - Train Loss: 0.9135, Train Acc: 0.9986, Top-1 Acc: 0.8904, Avg Class Acc: 0.8893\nEpoch 9/20 - Train Loss: 0.9022, Train Acc: 0.9990, Top-1 Acc: 0.8987, Avg Class Acc: 0.8960\nEpoch 10/20 - Train Loss: 0.8938, Train Acc: 0.9988, Top-1 Acc: 0.8929, Avg Class Acc: 0.8905\nEpoch 11/20 - Train Loss: 0.8872, Train Acc: 0.9992, Top-1 Acc: 0.8937, Avg Class Acc: 0.8910\nEpoch 12/20 - Train Loss: 0.8842, Train Acc: 0.9990, Top-1 Acc: 0.8937, Avg Class Acc: 0.8911\nEpoch 13/20 - Train Loss: 0.8797, Train Acc: 0.9994, Top-1 Acc: 0.8953, Avg Class Acc: 0.8932\nEarly stopping triggered after 13 epochs!\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-3-ea99478405fe>:172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"\nFinal Top-1 Accuracy: 0.8987\nFinal Average Per-Class Accuracy: 0.8960\n","output_type":"stream"}],"execution_count":3}]}